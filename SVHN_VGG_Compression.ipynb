{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmWw3JUy5YeVU8/2rwiuLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkordonis/TropicalML/blob/main/SVHN_VGG_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpFP2za_UuYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4505b42d-9952-43c3-97ba-3c001393b19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import cvxpy as cp\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "transform = transforms.Compose([ transforms.ToTensor(),\n",
        "                               transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "training_dataset = datasets.SVHN('./data_src', split='train', download=True, transform=transform)\n",
        "test_dataset = datasets.SVHN('./data_src', split='test', download=True, transform=transform)\n",
        "batch_size=32\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUxfj5rdU80V",
        "outputId": "4efdafe5-3253-4f50-97ae-aea7a768c68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data_src/train_32x32.mat\n",
            "Using downloaded and verified file: ./data_src/test_32x32.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG_like_Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG_like_Net, self).__init__()\n",
        "        self.layer1= nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.layer2= nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3= nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer4= nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4*4*128, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc_out= nn.Sequential(\n",
        "            nn.Linear(1024, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "model = VGG_like_Net().to(device)"
      ],
      "metadata": {
        "id": "pWz_wqOLVQxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/My Drive/SVHN_VGG.pth'\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "cqF1esxzlunN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f84a24-761b-49c1-fc44-6eedc21a5609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-db368024d950>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_batch(data, labels, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "1Sw2IbDZVO_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    def test_batch(data, labels, model, criterion):\n",
        "        model.eval()\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, labels)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "YC1ggkrWPztG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    def accuracy(data, labels, model):\n",
        "        model.eval()\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = model(data)\n",
        "        _, pred_labels = output.max(-1)\n",
        "        correct = (pred_labels == labels)\n",
        "        return correct.cpu().detach().numpy().tolist()"
      ],
      "metadata": {
        "id": "q11F2EfZP1Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = model.fc[1].weight.data.clone().detach().cpu().numpy()\n",
        "b1 = model.fc[1].bias.data.clone().detach().cpu().numpy()\n",
        "W2 = model.fc_out[0].weight.data.clone().detach().cpu().numpy()\n",
        "b2 = model.fc_out[0].bias.data.clone().detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "Ghgtm76hVwWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Sample_Pair_of_outp_to_Divide(W1_mat,W2,b1):\n",
        "  idx_Out_1, idx_Out_2 = random.sample(range(10), 2)\n",
        "\n",
        "  Coef = W2[idx_Out_1,:]-W2[idx_Out_2,:]\n",
        "  W_pos = Coef[Coef>=0,None]*W1[Coef>=0]\n",
        "  W_neg = Coef[Coef<0,None]*W1[Coef<0]\n",
        "  b_pos = Coef[Coef>=0]*b1[idx_Out_1]\n",
        "  b_neg = Coef[Coef<0]*b1[idx_Out_2]\n",
        "  return W_pos,W_neg,b_pos,b_neg\n",
        "\n",
        "W_pos,W_neg,b_pos,b_neg=Sample_Pair_of_outp_to_Divide(W1,W2,b1)"
      ],
      "metadata": {
        "id": "tt1bNut2WpLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Define the Division Function ##\n",
        "\n"
      ],
      "metadata": {
        "id": "YRaIhYDCXxwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Division_function(a_divident,b_divident,Iterations =5,m_q=2):\n",
        "  #Initialization Process: Start with almost the same number of samples per term\n",
        "  d=a_divident.shape[1]\n",
        "  a_hat, b_hat=np.random.randn(m_q,d), np.zeros(m_q)\n",
        "  for i in range(20):\n",
        "    I_i = np.argmax((a_hat@X_sample.T+np.array([b_hat]).T), axis=0)\n",
        "    b_hat=b_hat-np.array([(I_i==j).sum() for j in range(m_q)])*0.005\n",
        "    b_hat=b_hat-b_hat.mean()\n",
        "\n",
        "  #Compute the f values\n",
        "  f_x_i=np.max((a_divident@X_sample.T+np.array([b_divident]).T), axis=0)\n",
        "  m_p=a_divident.shape[0]\n",
        "  # Define cp variables\n",
        "  x_b, lambda_var =   cp.Variable(1), cp.Variable(m_p)\n",
        "  for cnt in range(Iterations):\n",
        "    I_i=np.argmax((a_hat@X_sample.T+np.array([b_hat]).T), axis=0)         # Find the sets I_i\n",
        "    N=np.array([(I_i==j).sum() for j in range(m_q)])                      # Compute the number of elements in each set I_i\n",
        "    s=np.array([np.sum(X_sample[I_i==i], axis=0) for i in range(m_q)])    # Compute the summation o x_j's\n",
        "    for i in range(a_hat.shape[0]):\n",
        "      # In this problem we substituted a_hat_i by a linear combination of a_i's\n",
        "      prob = cp.Problem(cp.Maximize(lambda_var@a_divident@s[i]+N[i]*x_b),\n",
        "                      [(X_sample@(lambda_var@a_divident))+x_b<=f_x_i,\n",
        "                      lambda_var>=0,\n",
        "                      cp.sum(lambda_var)==1,\n",
        "                      ])\n",
        "      prob.solve(warm_start=True)\n",
        "      a_hat[i]=(lambda_var@a_divident).value\n",
        "      b_hat[i]=x_b.value\n",
        "  return a_hat,b_hat"
      ],
      "metadata": {
        "id": "ZBDlmadTWpE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_output = None\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    global intermediate_output\n",
        "    intermediate_output = output\n",
        "\n",
        "hook_handle = model.layer6.register_forward_hook(hook_fn)\n",
        "X_sample=np.zeros([0,4*4*128])\n",
        "\n",
        "cnt=0\n",
        "for i in training_dataloader:\n",
        "  inputs = i[0].to(device)\n",
        "  output = model(inputs)\n",
        "  X_to_add = intermediate_output.reshape(batch_size, -1).detach().to('cpu').numpy()\n",
        "  X_sample=np.concatenate((X_sample,X_to_add),axis=0)\n",
        "  cnt+=1\n",
        "  if cnt>=7:\n",
        "    break\n",
        "\n",
        "X_sample = X_sample[:200]\n"
      ],
      "metadata": {
        "id": "Cn9oz9Fam3v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "T=time.time()\n",
        "\n",
        "a_hat_pos_list,b_hat_pos_list,a_hat_neg_list,b_hat_neg_list=[],[],[],[]  # Initialize the lists\n",
        "for number_of_terms in range(100):\n",
        "  W_pos,W_neg,b_pos,b_neg=Sample_Pair_of_outp_to_Divide(W1,W2,b1)\n",
        "  a_hat,b_hat=Division_function(W_pos,b_pos)\n",
        "  a_hat_pos_list.append(a_hat)\n",
        "  b_hat_pos_list.append(b_hat)\n",
        "  a_hat,b_hat=Division_function(W_neg,b_neg)\n",
        "  a_hat_neg_list.append(a_hat)\n",
        "  b_hat_neg_list.append(b_hat)\n",
        "  #print('iteration', number_of_terms,'Time',time.time()-T)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnN4wMn5rU7T",
        "outputId": "0dbf5bc4-694a-44a2-e466-4159cd7d0671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-82305a24b546>:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  b_hat[i]=x_b.value\n",
            "/usr/local/lib/python3.10/dist-packages/cvxpy/problems/problem.py:1407: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_hat_np_array_pos = np.array(a_hat_pos_list)\n",
        "b_hat_np_array_pos = np.array(b_hat_pos_list)\n",
        "a_hat_np_array_neg = np.array(a_hat_neg_list)\n",
        "b_hat_np_array_neg = np.array(b_hat_neg_list)\n",
        "\n",
        "model_path_folder = '/content/drive/My Drive/'\n",
        "\n",
        "np.save(model_path_folder+'SVHN_b_hat_np_array_pos.npy',b_hat_np_array_pos)\n",
        "np.save(model_path_folder+'SVHN_a_hat_np_array_neg.npy',a_hat_np_array_neg)\n",
        "np.save(model_path_folder+'SVHN_b_hat_np_array_neg.npy',b_hat_np_array_neg)\n"
      ],
      "metadata": {
        "id": "XUiyu1UMbitF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-LM0oeCO0tDl",
        "outputId": "1a8cbeec-8028-4c9d-82c8-9db7aa69982e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/SVHN_a_hat_np_array_pos.npy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_folder = '/content/drive/My Drive/'\n",
        "\n",
        "a_hat_np_array_pos = np.load(model_path_folder+'SVHN_b_hat_np_array_pos.npy')\n",
        "b_hat_np_array_pos = np.load(model_path_folder+'SVHN_b_hat_np_array_pos.npy')\n",
        "a_hat_np_array_neg = np.load(model_path_folder+'SVHN_a_hat_np_array_neg.npy')\n",
        "b_hat_np_array_neg = np.load(model_path_folder+'SVHN_b_hat_np_array_neg.npy')\n"
      ],
      "metadata": {
        "id": "w-1lAEOtkWLh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "9e433e30-638f-4a51-d56d-caf68c3ddb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'SVHN_a_hat_np_array_pos.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fe16a6cc0c67>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma_hat_np_array_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SVHN_a_hat_np_array_pos.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb_hat_np_array_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SVHN_b_hat_np_array_pos.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma_hat_np_array_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SVHN_a_hat_np_array_neg.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb_hat_np_array_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SVHN_b_hat_np_array_neg.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SVHN_a_hat_np_array_pos.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified NN #"
      ],
      "metadata": {
        "id": "_t7kNzoFbPpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Vectors_to_choose=np.zeros([0,2048])\n",
        "Corresp_bias=np.zeros([0])\n",
        "for list_var in range(len(a_hat_pos_list)):\n",
        "  Vectors_to_choose=np.append(Vectors_to_choose, a_hat_pos_list[list_var], axis=0).copy()\n",
        "  Corresp_bias=np.append(Corresp_bias, b_hat_pos_list[list_var], axis=0).copy()\n",
        "\n",
        "  Vectors_to_choose=np.append(Vectors_to_choose, a_hat_neg_list[list_var], axis=0).copy()\n",
        "  Corresp_bias=np.append(Corresp_bias, b_hat_pos_list[list_var], axis=0).copy()\n"
      ],
      "metadata": {
        "id": "Dt_rQws7g-gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------a--------------"
      ],
      "metadata": {
        "id": "D-70GhhaVhCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 3\n",
        "W_form_Division = Vectors_to_choose[:2*K]\n",
        "W_form_Division = torch.from_numpy((W_form_Division)).float()\n",
        "biass_from_Division = Corresp_bias[:2*K]\n",
        "biass_from_Division = torch.from_numpy(biass_from_Division).float()\n",
        "\n",
        "\n",
        "A_mat1 = torch.zeros(K, 2 * K).float()\n",
        "A_mat2 = torch.zeros(K, 2 * K).float()\n",
        "\n",
        "for i in range(K):\n",
        "    A_mat1[i,2*i] = 1\n",
        "    A_mat2[i,2*i+1] =  1\n",
        "\n",
        "class small_nn_head(nn.Module):\n",
        "    def __init__(self, number_of_tropical_terms=K):\n",
        "        super(small_nn_head, self).__init__()\n",
        "\n",
        "        # Define layers\n",
        "        self.Layer1 = nn.Linear(4 * 4 * 128, 2 * number_of_tropical_terms)\n",
        "        self.Lin_part1 = nn.Linear(2*number_of_tropical_terms, number_of_tropical_terms)\n",
        "        self.Lin_part2 = nn.Linear(2*number_of_tropical_terms, number_of_tropical_terms)\n",
        "        self.Layer2 = nn.Linear(3*number_of_tropical_terms,10)\n",
        "\n",
        "\n",
        "        self.Lin_part1.weight.data = A_mat1\n",
        "        self.Lin_part1.bias.data = torch.zeros(number_of_tropical_terms)\n",
        "        self.Lin_part2.weight.data = A_mat2\n",
        "        self.Lin_part2.bias.data = torch.zeros(number_of_tropical_terms)\n",
        "        self.Layer1.weight.data = W_form_Division\n",
        "        self.Layer1.bias.data = biass_from_Division\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        out = self.Layer1(x)\n",
        "        out1 = self.Lin_part1(out)\n",
        "        out2 = self.Lin_part2(out)\n",
        "        max_12 = torch.max(out1, out2)\n",
        "        out = torch.cat((out, max_12), dim=1)\n",
        "        out = self.Layer2(out)\n",
        "        return out\n",
        "\n",
        "# Create the model instance and move it to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_substitute = small_nn_head(number_of_tropical_terms=K)\n",
        "model_substitute.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v6TIoMyMrVZ",
        "outputId": "6d53c1ac-07d3-484d-9af0-9cbfb07d7123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "small_nn_head(\n",
              "  (Layer1): Linear(in_features=2048, out_features=6, bias=True)\n",
              "  (Lin_part1): Linear(in_features=6, out_features=3, bias=True)\n",
              "  (Lin_part2): Linear(in_features=6, out_features=3, bias=True)\n",
              "  (Layer2): Linear(in_features=9, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset for compressed Layer\n",
        "activation = {}\n",
        "def getActivation(name):\n",
        "  def hook(model, input, output):\n",
        "    activation[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "def set_hooks(INP_Layer):\n",
        "  hook_INP_to_Compressed_Layer = INP_Layer.register_forward_hook(getActivation('INP_to_Compressed_Layer'))\n",
        "\n",
        "class LayerComputeDataset(Dataset):\n",
        "    def __init__(self, Init_model,from_layer):\n",
        "        self.from_layer= from_layer\n",
        "        self.Init_model=Init_model\n",
        "        self.num_samples = training_dataset.data.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x= training_dataset.data[idx]\n",
        "        with torch.no_grad():\n",
        "#          self.Init_model(transforms.ToTensor()(x).unsqueeze(0).float().to(device))\n",
        "          self.Init_model(torch.tensor(x,dtype=torch.float32).unsqueeze(0).float().to(device))\n",
        "\n",
        "        input_data = activation['INP_to_Compressed_Layer']\n",
        "        labels = training_dataset.labels[idx]\n",
        "\n",
        "        return input_data.squeeze(), labels\n"
      ],
      "metadata": {
        "id": "7cFBach6Niil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_hooks( model.layer6)\n",
        "DataSetForCompressLayer=LayerComputeDataset(model,'INP_to_Compressed_Layer')\n",
        "DataLoaderForCompressLayer = DataLoader(DataSetForCompressLayer, batch_size=32, shuffle=True,num_workers=0)"
      ],
      "metadata": {
        "id": "kaZNRMefYBPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG_like_Net_COMPRESSED(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG_like_Net_COMPRESSED, self).__init__()\n",
        "        self.layer1= nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.layer2= nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3= nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer4= nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "\n",
        "\n",
        "        self.fc = small_nn_head(number_of_tropical_terms=K)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "Model_Compressed = VGG_like_Net_COMPRESSED().to(device)"
      ],
      "metadata": {
        "id": "v-0mRHWo-xEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q4nKp33fP-_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Compressed.layer1 = copy.deepcopy(model.layer1)\n",
        "Model_Compressed.layer2 = copy.deepcopy(model.layer2)\n",
        "Model_Compressed.layer3 = copy.deepcopy(model.layer3)\n",
        "Model_Compressed.layer4 = copy.deepcopy(model.layer4)\n",
        "Model_Compressed.layer5 = copy.deepcopy(model.layer5)\n",
        "Model_Compressed.layer6 = copy.deepcopy(model.layer6)\n",
        "\n"
      ],
      "metadata": {
        "id": "FvUOpiKW_FNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in Model_Compressed.parameters())/sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z5pY3n0V_3Z",
        "outputId": "7267f128-7be4-4889-aa8a-a3343ba25b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12533332220520546"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_train=model_substitute\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_to_train.parameters(), lr=0.001)\n",
        "\n",
        "n_epochs = 5\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    training_losses, test_losses = [], []\n",
        "    training_accuracies, test_accuracies = [], []\n",
        "    for data, labels in DataLoaderForCompressLayer:\n",
        "        trng_batch_loss = training_batch(data, labels, model_to_train, criterion, optimizer)\n",
        "        training_losses.append(trng_batch_loss.item())\n",
        "        trng_batch_correct = accuracy(data, labels, model_to_train)\n",
        "        training_accuracies.extend(trng_batch_correct)\n",
        "    training_per_epoch_loss = np.array(training_losses).mean()\n",
        "    training_per_epoch_accuracy = np.mean(training_accuracies)\n",
        "    mean_loss =[]\n",
        "    Model_Compressed.fc = copy.deepcopy(model_substitute)\n",
        "    for data, labels in test_dataloader:\n",
        "      TruthValue=torch.argmax(Model_Compressed(data.to(device)),axis=1)==labels.to(device)\n",
        "      mean_loss.append(TruthValue.sum()/labels.shape[0])\n",
        "\n",
        "    print(f'Epoch: {epoch+1}/{n_epochs}\\t| Training loss: {training_per_epoch_loss:.4f} | ', end='')\n",
        "    print(f'Test accuracy: {torch.tensor(mean_loss).mean():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il-1vYXTA6fM",
        "outputId": "6da7a818-4cb9-4554-fefc-faf58b71b539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5\t| Training loss: 0.8072 | Test accuracy: 0.8915\n",
            "Epoch: 2/5\t| Training loss: 0.3280 | Test accuracy: 0.9085\n",
            "Epoch: 3/5\t| Training loss: 0.2659 | Test accuracy: 0.9172\n",
            "Epoch: 4/5\t| Training loss: 0.2394 | Test accuracy: 0.9209\n",
            "Epoch: 5/5\t| Training loss: 0.2225 | Test accuracy: 0.9244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1t5NupTufYRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BOWvwbICfYKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_copy = copy.deepcopy(model)\n",
        "W1 = model_copy.fc[1].weight.detach()\n",
        "b1 = model_copy.fc[1].bias.detach()\n",
        "N=int(1024*(1-))\n",
        "NORMS=torch.tensor([torch.norm(W1[i]) for i in range(1024)])\n",
        "values, indices = torch.topk(NORMS, N, largest=False)\n",
        "W1[indices]=0\n",
        "b1[indices]=0\n",
        "\n",
        "\n",
        "model_copy.fc[1].weight.data.copy_(W1)\n",
        "model_copy.fc[1].bias.data = b1.clone()\n",
        "for data, labels in test_dataloader:\n",
        "      TruthValue=torch.argmax(model_copy(data.to(device)),axis=1)==labels.to(device)\n",
        "      mean_loss.append(TruthValue.sum()/labels.shape[0])\n",
        "print(torch.tensor(mean_loss).mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWERDxd4fXz0",
        "outputId": "00a3f636-39ba-47a7-e712-32907a2eb423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9207)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "(torch.tensor([torch.norm(W1[i]) for i in range(1024)])==0).sum()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cfQKLjCi6MX",
        "outputId": "af98d62a-c4f7-457d-c307-a1ac24f8b9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(921)"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    }
  ]
}