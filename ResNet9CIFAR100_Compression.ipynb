{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkordonis/TropicalML/blob/main/ResNet9CIFAR100_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC3LQ_G9tDTO",
        "outputId": "86b21537-259f-4028-b780-2bfd79626f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import *\n",
        "from google.colab import files\n",
        "import cvxpy as cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P40yz6eJtPkR"
      },
      "outputs": [],
      "source": [
        "batch_size = 400\n",
        "epochs = 120\n",
        "max_lr = 0.001\n",
        "grad_clip = 0.01\n",
        "weight_decay =0.001\n",
        "opt_func = torch.optim.Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7J65XVJtRPq",
        "outputId": "77cd5d92-7052-4b4e-e9e7-51b19d7e5867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_data = torchvision.datasets.CIFAR100('./', train=True, download=True)\n",
        "\n",
        "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
        "mean = np.mean(x, axis=(0, 1))/255\n",
        "std = np.std(x, axis=(0, 1))/255\n",
        "mean=mean.tolist()\n",
        "std=std.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyJ3pU9vtS7v"
      },
      "outputs": [],
      "source": [
        "transform_train = tt.Compose([tt.RandomCrop(32, padding=4,padding_mode='reflect'),\n",
        "                         tt.RandomHorizontalFlip(),\n",
        "                         tt.ToTensor(),\n",
        "                         tt.Normalize(mean,std,inplace=True)])\n",
        "transform_test = tt.Compose([tt.ToTensor(), tt.Normalize(mean,std)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj0R47D9tUrq",
        "outputId": "3b26b631-5c9f-4153-ac51-ef1062d44d7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR100(\"./\",  train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader( trainset, batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\"./\",  train=False,  download=True,   transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader( testset, batch_size*2,pin_memory=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0zl8tHEtWep",
        "outputId": "d5be7569-bea0-4d3b-ebc8-8e08dd019d86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXihAq6Zt_dQ"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d((1,1)),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "model = to_device(ResNet9(3, 100), device)\n",
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVIhUkvkWPmq",
        "outputId": "ee0e273a-853b-4cea-feae-7e4aeba8dd1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6621540"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        " sum(p.numel() for p in model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZK9SnPl_Aw4"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in test_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, test_loader,\n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            if grad_clip:\n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "\n",
        "        # Validation phase\n",
        "        result = evaluate(model, test_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0540QphuKow",
        "outputId": "4b32349b-ebca-4848-ddc9-389f8a632b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-b75ff0467607>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "model_path = '/content/drive/My Drive/ResNet9CIFAR100.pth'\n",
        "#model.load_state_dict(torch.load(model_path))\n",
        "#model.eval()\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eLL2JTM-Eoh"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "#evaluate(model, DeviceDataLoader(testloader, device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basHdI6y02Jm"
      },
      "outputs": [],
      "source": [
        "def include_bn2d_into_conv2d(batchnorm_m,conv2d_c):\n",
        "  bn_weight,bn_bias, bn_mean,bn_var, eps = batchnorm_m.weight,batchnorm_m.bias,  batchnorm_m.running_mean, batchnorm_m.running_var, batchnorm_m.eps\n",
        "  scale = bn_weight / torch.sqrt(bn_var + eps)\n",
        "  shift = bn_bias - bn_mean * scale\n",
        "  new_weight = (conv2d_c.weight * scale.view(-1, 1, 1, 1)).detach().clone().to('cpu').numpy()\n",
        "  new_bias = ((conv2d_c.bias * scale + shift)).detach().clone().to('cpu').numpy()\n",
        "  return new_weight,new_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAtAI5HqaIg4"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "W1,b1=include_bn2d_into_conv2d(model.res2[0][1],model.res2[0][0])\n",
        "W2,b2=include_bn2d_into_conv2d(model.res2[1][1],model.res2[1][0])\n",
        "W1_mat=W1.reshape(W1.shape[0],-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1j2XIWTqhsK"
      },
      "source": [
        "# Division Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7At2DbJe4KDC"
      },
      "outputs": [],
      "source": [
        "# Number of terms in the division\n",
        "m_q=2\n",
        "#dimension of the space\n",
        "d=512*3*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perTDeX5lHq-"
      },
      "outputs": [],
      "source": [
        "W1,b1=include_bn2d_into_conv2d(model.res2[0][1],model.res2[0][0])\n",
        "W2,b2=include_bn2d_into_conv2d(model.res2[1][1],model.res2[1][0])\n",
        "W1_mat=W1.reshape(W1.shape[0],-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAPaIc36J1dP",
        "outputId": "646a70cb-db24-4fb8-f4d3-f8e10c900c71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((512, 4608), (512, 512, 3, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "W1_mat.shape,W2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPqfBTZs5nUV"
      },
      "outputs": [],
      "source": [
        "def Sample_Pair_of_outp_to_Divide(W1_mat,W2,b1):\n",
        "  idx_x1, idx_y1, idx_plane1=np.random.randint(0,3), np.random.randint(0,3), np.random.randint(0,512)\n",
        "  idx_x2, idx_y2, idx_plane2=np.random.randint(0,3), np.random.randint(0,3), np.random.randint(0,512)\n",
        "\n",
        "  Coef = W2[idx_plane1,:,idx_x1,idx_y1]-W2[idx_plane2,:,idx_x2,idx_y2]\n",
        "  W_pos = Coef[Coef>=0,None]*W1_mat[Coef>=0]\n",
        "  W_neg = Coef[Coef<0,None]*W1_mat[Coef<0]\n",
        "  b_pos = Coef[Coef>=0]*b1[idx_plane1]\n",
        "  b_neg = Coef[Coef<0]*b1[idx_plane2]\n",
        "  return W_pos,W_neg,b_pos,b_neg\n",
        "\n",
        "W_pos,W_neg,b_pos,b_neg=Sample_Pair_of_outp_to_Divide(W1_mat,W2,b1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOvoy3PP8faP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3EXMKpSqfjt"
      },
      "outputs": [],
      "source": [
        "# Collect the output of the intermediate layer\n",
        "\n",
        "intermediate_output = None\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    global intermediate_output\n",
        "    intermediate_output = output\n",
        "\n",
        "hook_handle = model.conv4[2].register_forward_hook(hook_fn)\n",
        "X_sample=np.zeros([0,512*9])\n",
        "\n",
        "cnt=0\n",
        "for i in trainloader:\n",
        "  inputs = i[0].to(device)\n",
        "  output = model(inputs)\n",
        "  x_smpl,y_smpl=np.random.randint(1,7),np.random.randint(1,7)\n",
        "  X_to_add = ((intermediate_output[:,:,x_smpl-1:x_smpl+2,y_smpl-1:y_smpl+2]).detach().reshape(400,512*9)).to('cpu').numpy()\n",
        "  X_sample=np.concatenate((X_sample,X_to_add),axis=0)\n",
        "  cnt+=1\n",
        "  if cnt>=5:\n",
        "    break\n",
        "\n",
        "X_sample = X_sample[:200]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq6Ym-yaqfhQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgSEu_Po9DKl"
      },
      "outputs": [],
      "source": [
        "# Define the Division Function\n",
        "def Division_function(a_divident,b_divident,Iterations =5):\n",
        "  #Initialization Process: Start with almost the same number of samples per term\n",
        "  a_hat, b_hat=np.random.randn(m_q,d), np.zeros(m_q)\n",
        "  for i in range(20):\n",
        "    I_i = np.argmax((a_hat@X_sample.T+np.array([b_hat]).T), axis=0)\n",
        "    b_hat=b_hat-np.array([(I_i==j).sum() for j in range(m_q)])*0.005\n",
        "    b_hat=b_hat-b_hat.mean()\n",
        "\n",
        "  #Compute the f values\n",
        "  f_x_i=np.max((a_divident@X_sample.T+np.array([b_divident]).T), axis=0)\n",
        "  m_p=a_divident.shape[0]\n",
        "  # Define cp variables\n",
        "  x_b, lambda_var =   cp.Variable(1), cp.Variable(m_p)\n",
        "  for cnt in range(Iterations):\n",
        "    I_i=np.argmax((a_hat@X_sample.T+np.array([b_hat]).T), axis=0)         # Find the sets I_i\n",
        "    N=np.array([(I_i==j).sum() for j in range(m_q)])                      # Compute the number of elements in each set I_i\n",
        "    s=np.array([np.sum(X_sample[I_i==i], axis=0) for i in range(m_q)])    # Compute the summation o x_j's\n",
        "    for i in range(a_hat.shape[0]):\n",
        "      # In this problem we substituted a_hat_i by a linear combination of a_i's\n",
        "      prob = cp.Problem(cp.Maximize(lambda_var@a_divident@s[i]+N[i]*x_b),\n",
        "                      [(X_sample@(lambda_var@a_divident))+x_b<=f_x_i,\n",
        "                      lambda_var>=0,\n",
        "                      cp.sum(lambda_var)==1,\n",
        "                      ])\n",
        "      prob.solve(warm_start=True)\n",
        "      a_hat[i]=(lambda_var@a_divident).value\n",
        "      b_hat[i]=x_b.value\n",
        "  return a_hat,b_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGMhmDJHqfRJ",
        "outputId": "8bbea3a2-30d3-437c-feae-476c42cd91a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-88-62e5ebb6f17e>:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  b_hat[i]=x_b.value\n"
          ]
        }
      ],
      "source": [
        "# Iterative Pairwise Divisions\n",
        "a_hat_pos_list,b_hat_pos_list,a_hat_neg_list,b_hat_neg_list=[],[],[],[]  # Initialize the lists\n",
        "for number_of_terms in range(100):\n",
        "  W_pos,W_neg,b_pos,b_neg=Sample_Pair_of_outp_to_Divide(W1_mat,W2,b1)\n",
        "  a_hat,b_hat=Division_function(W_pos,b_pos)\n",
        "  a_hat_pos_list.append(a_hat)\n",
        "  b_hat_pos_list.append(b_hat)\n",
        "  a_hat,b_hat=Division_function(W_neg,b_neg)\n",
        "  a_hat_neg_list.append(a_hat)\n",
        "  b_hat_neg_list.append(b_hat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRvMFgQlqfOa"
      },
      "outputs": [],
      "source": [
        "Vectors_to_choose=np.zeros([0,512*9])\n",
        "Corresp_bias=np.zeros([0])\n",
        "for list_var in range(len(a_hat_pos_list)):\n",
        "  Vectors_to_choose=np.append(Vectors_to_choose, a_hat_pos_list[list_var], axis=0).copy()\n",
        "  Corresp_bias=np.append(Corresp_bias, b_hat_pos_list[list_var], axis=0).copy()\n",
        "\n",
        "  Vectors_to_choose=np.append(Vectors_to_choose, a_hat_neg_list[list_var], axis=0).copy()\n",
        "  Corresp_bias=np.append(Corresp_bias, b_hat_pos_list[list_var], axis=0).copy()\n",
        "\n",
        "DivisionResultsPath_Vectors_to_choose='/content/drive/My Drive/Vectors_to_choose.pt'\n",
        "DivisionResultsPath_Corresp_bias = '/content/drive/My Drive/Corresp_bias.pt'\n",
        "\n",
        "torch.save(Vectors_to_choose, DivisionResultsPath_Vectors_to_choose)\n",
        "torch.save(Corresp_bias, DivisionResultsPath_Corresp_bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsN1RlGQ9FoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596422e2-1d14-4a24-e419-b2d1756cf51a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 4608)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "Vectors_to_choose.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EWSY-upLDjG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvQb07CEA0M1"
      },
      "source": [
        "# Create a new NN  #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5fcg3ta61mR"
      },
      "outputs": [],
      "source": [
        "K=25\n",
        "W_mat_conv_1_new = Vectors_to_choose[:2*K]\n",
        "W_mat_conv_1_new=torch.from_numpy((W_mat_conv_1_new.reshape((-1,512, 3, 3))))\n",
        "biass_conv_1_new = Corresp_bias[:2*K]\n",
        "biass_conv_1_new=torch.from_numpy(biass_conv_1_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2iF26mqDTI3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0YRVxfvBBJw"
      },
      "outputs": [],
      "source": [
        "# Create the substitute for the residual block\n",
        "A_mat1 = torch.zeros(K,2*K,1,1)\n",
        "A_mat2 = torch.zeros(K,2*K,1,1)\n",
        "\n",
        "for i in range(K):\n",
        "  A_mat1[i,2*i,0,0]=1\n",
        "  A_mat2[i,2*i+1,0,0]=1\n",
        "\n",
        "\n",
        "class Subs_Res_Block(nn.Module):\n",
        "    def __init__(self, Dim1, Dim2,Dim3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(Dim1, Dim2,kernel_size=(3, 3),padding=(1,1))\n",
        "        self.conv1.weight.data.copy_(W_mat_conv_1_new)\n",
        "        self.conv1.bias.data.copy_(biass_conv_1_new)\n",
        "\n",
        "        self.fixed_conv_part_1 = nn.Conv2d(Dim2, Dim2//2,kernel_size=(1, 1),bias=False )\n",
        "        self.fixed_conv_part_1.weight.data.copy_(A_mat1)\n",
        "        self.fixed_conv_part_2 = nn.Conv2d(Dim2, Dim2//2,kernel_size=(1, 1),bias=False)\n",
        "        self.fixed_conv_part_2.weight.data.copy_(A_mat2)\n",
        "        self.conv3 = nn.Conv2d(Dim2+Dim2//2, Dim3,kernel_size=(3, 3), padding=(1,1))\n",
        "\n",
        "\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        one = self.fixed_conv_part_1(out)\n",
        "        two = self.fixed_conv_part_2(out)\n",
        "        out = self.relu1(out)\n",
        "        out = torch.concatenate((out,torch.max(one,two)),axis=1)\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "    def training_step(self, batch):\n",
        "        Inps, Outps = batch\n",
        "        Outp_preds = self(Inps)                  # Generate predictions\n",
        "        loss = F.mse_loss(Outp_preds  , Outps)  # Calculate loss\n",
        "        return loss\n",
        "\n",
        "model_small_new = Subs_Res_Block(512,2*K,512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZGRzW3c2_gE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71cd21a-bf7d-43fe-f602-272ba1a9082c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12263939153654305"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "sum(p.numel() for p in model_small_new.parameters())/sum(p.numel() for p in model.res2.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyurHmd5_le5"
      },
      "outputs": [],
      "source": [
        "# Create Dataset for compressed Layer\n",
        "activation = {}\n",
        "def getActivation(name):\n",
        "  def hook(model, input, output):\n",
        "    activation[name] = output.detach()\n",
        "  return hook\n",
        "\n",
        "def set_hooks(INP_Layer,OUTP_Layer):\n",
        "  # register forward hooks on the layers\n",
        "  hook_INP_to_Compressed_Layer =INP_Layer.register_forward_hook(getActivation('INP_to_Compressed_Layer'))\n",
        "  hook_OUTP_of_Compressed_Layer = OUTP_Layer.register_forward_hook(getActivation('OUTP_of_Compressed_Layer'))\n",
        "\n",
        "class LayerComputeDataset(Dataset):\n",
        "    def __init__(self, Res_model,from_layer, to_layer, num_samples):\n",
        "        self.from_layer= from_layer\n",
        "        self.to_layer= to_layer\n",
        "        self.Res_model=Res_model\n",
        "        self.num_samples = train_data.data.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x= train_data.data[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "          self.Res_model(tt.ToTensor()(x).unsqueeze(0).float().to(device))\n",
        "        input_data = activation['INP_to_Compressed_Layer']\n",
        "\n",
        "        return input_data.squeeze(), activation['OUTP_of_Compressed_Layer'].squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di6lYp_ByD8h"
      },
      "outputs": [],
      "source": [
        "set_hooks( model.conv4,model.res2)\n",
        "DataSetForCompressLayer=LayerComputeDataset(model,'INP_to_Compressed_Layer','OUTP_of_Compressed_Layer',50000)\n",
        "DataLoaderForCompressLayer = DataLoader(DataSetForCompressLayer, batch_size=32, shuffle=True,num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q80XufMNRZqM"
      },
      "outputs": [],
      "source": [
        "Sub_Res_Model = Subs_Res_Block(Dim1=512, Dim2=2*K, Dim3=512)\n",
        "Sub_Res_Model = to_device(Sub_Res_Model,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_TgJ8sTD-7l"
      },
      "outputs": [],
      "source": [
        "class Extended_ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes,Type_of_Substitution,Interm_Number):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        if Type_of_Substitution == \"Subs_Res_Block_v1\":\n",
        "          self.res2_mod = Subs_Res_Block(512, Interm_Number, 512)\n",
        "        else:\n",
        "          error\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d((1,1)),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2_mod(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "Model_Extended = Extended_ResNet9(3,100,\"Subs_Res_Block_v1\",Interm_Number=2*K)\n",
        "Model_Extended.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-sruvLNeFxt"
      },
      "outputs": [],
      "source": [
        "def copy_from_model(model, Subst_LAYER,Model_Extended):\n",
        "  Dict1= model.state_dict()\n",
        "  Dict2= Model_Extended.state_dict()\n",
        "  for key in Dict1:\n",
        "      if key in Dict2:\n",
        "        Dict2[key]=Dict1[key]\n",
        "  Model_Extended.load_state_dict(Dict2)\n",
        "  Model_Extended.res2_mod.load_state_dict(Subst_LAYER.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msiU5Ux1j2Nz"
      },
      "outputs": [],
      "source": [
        "lossses=np.zeros(85001)\n",
        "lossses_2=np.zeros(85001)\n",
        "indexx=0\n",
        "\n",
        "optimizer = torch.optim.Adam(Sub_Res_Model.parameters(), lr=0.0001)\n",
        "\n",
        "for iterations in range(25):\n",
        "  idx=0\n",
        "  T=time.time()\n",
        "  for batch in DataLoaderForCompressLayer:\n",
        "      idx+=1\n",
        "      optimizer.zero_grad()\n",
        "      indexx+=1\n",
        "      loss = Sub_Res_Model.training_step(batch)\n",
        "      lossses[indexx]=loss.cpu().detach().numpy()\n",
        "      loss.backward()\n",
        "      lossses_2[idx]=loss.cpu().detach().numpy()\n",
        "      optimizer.step()\n",
        "      if indexx ==5009:\n",
        "        break\n",
        "  print(lossses_2[:idx].mean())\n",
        "  copy_from_model(model, Sub_Res_Model,Model_Extended)\n",
        "  print('valid_acc',evaluate(Model_Extended, DeviceDataLoader(testloader, device))['val_acc'])\n",
        "  print(time.time()-T)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mTjZT9-3oRj"
      },
      "source": [
        "L1 structured prunning comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxsfeCd1hAit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "4553e03d-1657-4c1b-e196-cf6a37520e0f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'N' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-3adf50239555>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
          ]
        }
      ],
      "source": [
        "N/512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IAdO1AX1GXi"
      },
      "outputs": [],
      "source": [
        "model_copy = copy.deepcopy(model)\n",
        "W1 = model_copy.res2[0][0].weight.detach()\n",
        "W2 = model_copy.res2[1][0].weight.detach()\n",
        "b1 = model_copy.res2[0][0].bias.detach()\n",
        "b2 = model_copy.res2[1][0].bias.detach()\n",
        "N=int(512*(1-0.35))\n",
        "NORMS=torch.tensor([torch.norm(W1[i]) for i in range(512)])\n",
        "values, indices = torch.topk(NORMS, N, largest=False)\n",
        "W1[indices]=0\n",
        "b1[indices]=0\n",
        "NORMS=torch.tensor([torch.norm(W2[i]) for i in range(512)])\n",
        "values, indices = torch.topk(NORMS, N, largest=False)\n",
        "W2[indices]=0\n",
        "b2[indices]=0\n",
        "\n",
        "model_copy.res2[0][0].weight.data.copy_(W1)\n",
        "model_copy.res2[0][0].bias.data.copy_(b1)\n",
        "model_copy.res2[1][0].weight.data.copy_(W2)\n",
        "model_copy.res2[1][0].bias.data.copy_(b2)\n",
        "\n",
        "print('valid_acc',evaluate(model_copy, DeviceDataLoader(testloader, device))['val_acc'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW5y8nLoiG5P"
      },
      "outputs": [],
      "source": [
        "print('valid_acc',evaluate(model, DeviceDataLoader(testloader, device))['val_acc'])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPDlHeNAMVSHEuLVJiXIxFT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}